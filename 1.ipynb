{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with a 'timestamp' column\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['timestamp'] = df['date'].dt.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "\n",
    "import catboost as cb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    data, target = load_breast_cancer(return_X_y=True)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\n",
    "            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "        ),\n",
    "        \"used_ram_limit\": \"3gb\",\n",
    "        \"eval_metric\": \"Accuracy\",\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n",
    "\n",
    "    gbm = cb.CatBoostClassifier(**param)\n",
    "\n",
    "    pruning_callback = CatBoostPruningCallback(trial, \"Accuracy\")\n",
    "    gbm.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        eval_set=[(valid_x, valid_y)],\n",
    "        verbose=0,\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[pruning_callback],\n",
    "    )\n",
    "\n",
    "    # evoke pruning manually.\n",
    "    pruning_callback.check_pruned()\n",
    "\n",
    "    preds = gbm.predict(valid_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = accuracy_score(valid_y, pred_labels)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "import lightgbm as lgb\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "def objective(trial):\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    dvalid = lgb.Dataset(valid_x, label=valid_y)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n",
    "    gbm = lgb.train(param, dtrain, valid_sets=[dvalid], callbacks=[pruning_callback])\n",
    "\n",
    "    preds = gbm.predict(valid_x)\n",
    "    return sklearn.metrics.roc_auc_score(valid_y, preds)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "def objective(trial):\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
    "    bst = xgb.train(param, dtrain, evals=[(dvalid, \"validation\")], callbacks=[pruning_callback])\n",
    "    preds = bst.predict(dvalid)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "estimators = []\n",
    "v = VotingClassifier(estimators=zip(range(len(estimators)), estimators))\n",
    "\n",
    "v.estimators_ = estimators\n",
    "v.classes_ = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
